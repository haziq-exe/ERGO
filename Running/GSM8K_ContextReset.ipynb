{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1sh0lbQ7LWqMFVHPb9mrywIcRGuThqku-","timestamp":1748807084273}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Load Model, Dataset"],"metadata":{"id":"YnldQj6j38cP"}},{"cell_type":"code","source":["from google.colab import drive\n","import json\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EsGHBhlWK0l","executionInfo":{"status":"ok","timestamp":1750498141850,"user_tz":-240,"elapsed":25619,"user":{"displayName":"HMK","userId":"16235041692412471169"}},"outputId":"80cf123c-51b6-4e85-8287-d46628cbbacb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open(\"/content/drive/My Drive/sharded_dataset.json\") as f:\n","  data = json.load(f)"],"metadata":{"id":"PsQP-SzxWOCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GSM8K = []\n","for task in data:\n","  if \"math\" in task['task']:\n","    GSM8K.append(task)"],"metadata":{"id":"yTcdnSbPX_aA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# OPENAI CODE"],"metadata":{"id":"J67WBHVo0i-h"}},{"cell_type":"code","source":["def eval_GSM8K(model_output, ground_truth):\n","  model_answer = re.findall(r'<Answer>\\s*(.*?)(?:</Answer>|$)', model_output, re.DOTALL)\n","  answer = re.findall(r'####\\s*(.*)', ground_truth)\n","\n","  if answer and model_answer:\n","    answer[0] = answer[0].replace(\",\", \"\")\n","    if answer[0] in model_answer[0]:\n","      print(\"Correct.\")\n","      return True\n","\n","\n","  return False"],"metadata":{"id":"3FXXt9iVkT4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def with_context_reset_chat(dataset, file_path, threshold,\n","                            temperature=1.0, runs=1, numQ=50):\n","    base_system = {\n","        \"role\": \"system\",\n","        \"content\": (\n","            \"You are a helpful math assistant. \"\n","            \"Put your final answer between <Answer> tags.\"\n","        )\n","    }\n","    connectors = [\"oh also, \", \"I just remembered, \", \"sorry i forgot to say, \", \"\", \"oh, and \", \"FYI, \"]\n","    tokens_used = 0\n","\n","    for run in range(runs):\n","        print(f\"Run {run+1}/{runs}\")\n","        out_path = file_path.replace(\".json\", f\"_run{run}.json\")\n","        results = []\n","\n","        for entry in dataset:\n","            shards = entry[\"shards\"]\n","            print(f\"Question with {len(shards)} shards\")\n","            messages = [base_system]\n","            prev_entropy = float(\"inf\")\n","            resets = 0\n","            entropies = []\n","            before_reset = None\n","            choice = random.choice(connectors)\n","\n","            for shard in shards:\n","                user_content = shard[\"shard\"]\n","                if shard['shard_id'] != 1:\n","                  user_content = choice + user_content\n","                if shard[\"shard_id\"] == len(shards):\n","                    user_content += \" Please put your final answer between <Answer> tags.\"\n","                messages.append({\"role\": \"user\", \"content\": user_content})\n","\n","                entropy, reply, tok = predictive_entropy_uncertainty_chat(messages, temperature=1.0, logprobs=True)\n","                print(f\"Entropy: {entropy:.4f}\")\n","                tokens_used += tok\n","\n","                if entropy - prev_entropy > threshold:\n","                    before_reset = list(messages)\n","                    messages = prompt_rewrite(messages)\n","                    entropy, reply, tok = predictive_entropy_uncertainty_chat(messages, temperature=0.2, logprobs=True)\n","                    tokens_used += tok\n","                    messages = [base_system, {\"role\": \"user\", \"content\": reply}]\n","                    entropy, reply, tok = predictive_entropy_uncertainty_chat(messages, temperature=1.0, logprobs=True)\n","                    print(f\"Reset entropy: {entropy:.4f}\")\n","                    tokens_used += tok\n","                    resets += 1\n","\n","                prev_entropy = entropy\n","                entropies.append(entropy)\n","                messages.append({\"role\": \"assistant\", \"content\": reply})\n","\n","                print(f\"Current Tokens Used: {tokens_used}\")\n","\n","                is_correct = eval_GSM8K(reply, entry['answer'])\n","\n","                if is_correct or shard[\"shard_id\"] == len(entry[\"shards\"]):\n","\n","                  if before_reset:\n","                    chat_history = f\"{before_reset}\\n\\nAFTER RESET\\n\\n{messages}\"\n","                  else:\n","                    chat_history = messages\n","\n","                  new_entry = {\"final_output\": reply, \"chat_history\": chat_history, \"entropies\": entropies, \"resets\":resets, \"correct\":is_correct}\n","\n","                  if os.path.exists(out_path):\n","                    with open(out_path, \"r\") as f:\n","                        data = json.load(f)\n","                  else:\n","                      data = []\n","\n","                  data.append(new_entry)\n","\n","                  with open(out_path, \"w\") as f:\n","                      json.dump(data, f, indent=2)\n","\n","                  break\n","\n","        print(f\"Saved: {out_path}\")"],"metadata":{"id":"kHsjNFHx0k9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import os\n","\n","def prompt_rewrite(prompt):\n","\n","  new_prompt = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are a prompt rewriter whose main goal is to rewrite the prompt given by the user in the most optimal way without losing any information in them.\"\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": (\n","            \"I have a set of questions and/or statements, please REWRITE all the questions/statements so that they are in the most optimal order that is the easiest to understand. DO NOT ANSWER ANY OF THE QUESTIONS JUST REWRITE. Here are the instructions:\\n\"\n","            \"how much will John pay in sales tax?\\n\"\n","            \"the purchase amount is $120\\n\"\n","            \"the sales tax rate is 8.25%. Put your final answer between <Answer> tags.\"\n","        )\n","    },\n","    {\n","        \"role\": \"assistant\",\n","        \"content\": \"What is the total sales tax John will pay on a $120 purchase at an 8.25% rate? Put your final answer between <Answer> tags.\"\n","    },\n","]\n","\n","  user_content = \"I have a set of questions and/or statements, please REWRITE all the questions/statements so that they are in the most optimal order that is the easiest to understand. DO NOT ANSWER ANY OF THE QUESTIONS JUST REWRITE. Here are the instructions:\\n\"\n","  user_messages = [item[\"content\"] for item in prompt if item.get(\"role\") == \"user\"]\n","  for msg in user_messages:\n","    user_content += msg + \"\\n\"\n","\n","  new_prompt.append({\"role\": \"user\", \"content\": user_content})\n","\n","  return new_prompt"],"metadata":{"id":"uza7AtBe1w2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","import math\n","\n","def predictive_entropy_uncertainty_chat(prompt,\n","                                         temperature=1.0,\n","                                         max_tokens = 500,\n","                                         logprobs= True):\n","    \"\"\"\n","    Send `prompt` to OpenAI Completion API, return average token entropy and generated text.\n","    Entropy is approximated from top `logprobs` returned per token.\n","    \"\"\"\n","\n","    client = OpenAI(api_key=\"FILLER\")\n","    resp = client.chat.completions.create(\n","        model=\"gpt-4.1\",\n","        messages=prompt,\n","        max_completion_tokens=max_tokens,\n","        temperature=temperature,\n","        logprobs=logprobs,\n","        top_logprobs = 20,\n","    )\n","\n","    generated_tokens = resp.choices[0].message.content\n","    lps = resp.choices[0].logprobs.content\n","\n","    entropies = []\n","\n","    for token_info in lps:\n","        entropy = 0.0\n","        for alt in token_info.top_logprobs:\n","            p = math.exp(alt.logprob)\n","            entropy += -p * alt.logprob\n","        entropies.append(entropy)\n","\n","    avg_entropy = sum(entropies) / len(entropies) if entropies else 0.0\n","\n","    tokens_used = resp.usage.completion_tokens + resp.usage.prompt_tokens\n","\n","    return avg_entropy, generated_tokens, tokens_used"],"metadata":{"id":"Vx2BD9Hk-0pj"},"execution_count":null,"outputs":[]}]}